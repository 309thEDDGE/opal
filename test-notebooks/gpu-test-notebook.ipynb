{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4571ef-1211-487d-90a2-02fc58c242ff",
   "metadata": {},
   "source": [
    "<h1>GPU test notebook using PyTorch and CUDA</h1>\n",
    "This GPU test notebook is designed to showcase the output of a system equipped with a GPU and running the OPAL software.\n",
    "\n",
    "<h3>What is PyTorch</h3>\n",
    "PyTorch is an open source marchine learning framework that enables you to perform scientific and tensor computations.  PyTorch can be used to speed up deep learning with the addition of a GPU or Graphics Processing Unit.  It is an open-source deep learning framework built on Python.  It is know for being flexible and easy to use. Here is what Pytorch can offer:\n",
    "\n",
    "- **Deep learning focus:** PyTorch is particularly well-suited for building deep learning models, a type of artificial intelligence inspired by the structure and function of the brain. These models are used in various applications like image recognition, natural language processing, and recommendation systems.\n",
    "- **Pythonic nature:** Since it's written in Python, a widely used high-level programming language, PyTorch is considered beginner-friendly for those already familiar with Python. This makes it easier to learn and use compared to some other deep learning frameworks.\n",
    "- **Dynamic computational graphs:** One of PyTorch's strengths is its use of dynamic computational graphs. These graphs define the relationships between different parts of a model and how data flows through it. PyTorch allows for these graphs to be modified on the fly, which is helpful for fast experimentation and prototyping.\n",
    "- **GPU support:** For demanding computations, PyTorch has excellent support for graphics processing units (GPUs). GPUs can significantly accelerate training deep learning models.\n",
    "\n",
    "Pytorch Documentation : [Pytorch Documentation](https://pytorch.org/get-started/locally/)\n",
    "\n",
    "<h3>What is CUDA</h3>\n",
    "CUDA is a programming model and computing toolkit developed by NVIDIA.  It takes advantage of the power of GPUs for general computing tasks, especially computations that can be parallelized or broken down into many smaller tasks that can be done at the same time.\n",
    "\n",
    "- **Parallel computing platform:** CUDA provides a way to write programs that can be run on multiple cores in a GPU simultaneously. This can be a huge speedup for tasks that can be broken down into many smaller pieces.\n",
    "\n",
    "- **Programming model:** CUDA includes a set of tools and libraries that make it easier to write programs that run on GPUs. This includes a special dialect of C/C++ that allows you to write code that can be run on the GPU.\n",
    "\n",
    "<h3>CUDA can also be used for</h3>\n",
    "\n",
    "- **Machine Learning:** Machine learning algorithms often involve a lot of mathematical computations that are well-suited to GPUs.\n",
    "- **Scientific computing:** Simulations and other scientific calculations can be accelerated with CUDA.\n",
    "- **Video editing and processing:** Many video editing and processing applications use CUDA to improve performance.\n",
    "- **Cryptocurrency mining:** Some cryptocurrencies can be mined more efficiently using GPUs with CUDA.\n",
    "\n",
    "CUDA Documentation : [CUDA Toolkit Documentation](https://docs.nvidia.com/cuda/) \n",
    "\n",
    "<h3>How does pytorch and CUDA work together</h3>\n",
    "\n",
    "You can think of CUDA as the hardware accelerator. CUDA unlocks the power of the GPU for interfaces used by the user.  In this instance pytoch is the user-friendly interface.  PyTorch bridges the gap between the python code and the underlying CUDA capabilities.  It provides a user-friendly API with the ```torch.cuda``` library to manage ensors (data structures) on the GPU.\n",
    "\n",
    "- **Tensor Placement:** You can create tensors on the GPU using ```device='cuda'``` argument during creation or move existing tensors using the ```.to('cuda')``` method. PyTorch automatically keeps track of which GPU you're using and assigns tensors to that device.\n",
    "- **Computation Acceleration:** Once your tensors reside on the GPU, PyTorch leverages CUDA to perform computations in parallel across the GPU cores. This significantly accelerates operations compared to the CPU.\n",
    "- **Data Transfer:** There's some overhead involved in transferring data between CPU and GPU memory. PyTorch manages these transfers efficiently.\n",
    "\n",
    "Pytorch CUDA documentation : [PyTorch documentation using CUDA](https://pytorch.org/docs/stable/cuda.html)\n",
    "\n",
    "<h2>Getting started</h2>\n",
    "First it is important to check your GPU system.  Using the NVIDIA System Management Interface SMI.  This command is intended to aid in the management and monitoring of NVIDA GPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30bffa3-eb15-41fb-bd6c-5fa06571307f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 15:34:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   20C    P0              35W / 300W |      0MiB / 16384MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bf455-515e-4768-90cc-56b6ca11d976",
   "metadata": {},
   "source": [
    "This will give you a quick printout that lists important information about your GPU.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389bb45-43dd-4cd8-b04e-0f8550e13cbb",
   "metadata": {},
   "source": [
    "<h4>Now lets use PyTorch to test CUDA</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0a806d-f1a0-47e0-bc2f-e260701a91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports are always needed when using PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b7c80c-0d68-4633-a21b-36e29ec5f104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if the gpu is available.  If this call returns false, you have an issue with your GPU configuration\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c22d4ef-3a22-4baa-ab63-7b9220e21fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the GPU index or ID\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4153be63-fc90-427c-a03d-9b4657e76a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of current GPUs on the system\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bfc57b-67cb-4196-bd0a-bdc9eb802a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-SXM2-16GB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the name of the GPU\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52e25832-2cba-46f8-a040-cca58d2c38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function might come in handy if you are running into memory issues while using PyTorch with CUDA\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c2ff85-fda8-4204-917b-a26f6705633b",
   "metadata": {},
   "source": [
    "Now that we have some basic knowledge of the GPU using PyTorch, we can run some tests to compare the difference a GPU can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffd1e0b8-7ec8-43ef-a4cc-0451283aa536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time =  12.13370656967163\n",
      "GPU time =  4.866923570632935\n"
     ]
    }
   ],
   "source": [
    "# this test will pit the CPU vs the GPU using a tensor filles with the scalar value 1, with the shape defined by the variable argument\n",
    "\n",
    "import torch\n",
    "import time\n",
    " \n",
    "###CPU\n",
    "start_time = time.time()\n",
    "a = torch.ones(400,400)\n",
    "for _ in range(1000000):\n",
    "    a += a\n",
    "elapsed_time = time.time() - start_time\n",
    " \n",
    "print('CPU time = ',elapsed_time)\n",
    " \n",
    "###GPU\n",
    "start_time = time.time()\n",
    "b = torch.ones(400,400).cuda()\n",
    "for _ in range(1000000):\n",
    "    b += b\n",
    "elapsed_time = time.time() - start_time\n",
    " \n",
    "print('GPU time = ',elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c073149d-98b4-43a6-93d2-130c8ac51be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 366.05731201171875\n",
      "199 247.44046020507812\n",
      "299 168.3035888671875\n",
      "399 115.47830200195312\n",
      "499 80.19639587402344\n",
      "599 56.618072509765625\n",
      "699 40.85108947753906\n",
      "799 30.30082130432129\n",
      "899 23.23638916015625\n",
      "999 18.50265121459961\n",
      "1099 15.328434944152832\n",
      "1199 13.1982421875\n",
      "1299 11.767601013183594\n",
      "1399 10.805967330932617\n",
      "1499 10.159031867980957\n",
      "1599 9.72340202331543\n",
      "1699 9.429821968078613\n",
      "1799 9.231765747070312\n",
      "1899 9.098038673400879\n",
      "1999 9.007640838623047\n",
      "Result: y = -0.009008153341710567 + 0.8673025369644165 x + 0.0015540558379143476 x^2 + -0.09483269602060318 x^3\n"
     ]
    }
   ],
   "source": [
    "# this test can be ran with the CPU or GPU.  It uses the RANDN function to return a tensor filled with random numbers from a normal distribution\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb777d-034e-432e-9069-a8a1c07e522a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:singleuser] *",
   "language": "python",
   "name": "conda-env-singleuser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
